# -*- coding: utf-8 -*-
"""ModelAgnostikExplainabilitywithLIME(MyApproach).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ix-u-zzjhVVoM2xwk9MvSw548UxeMCa3

https://api.lexicala.com/documentation

https://rapidapi.com/kdictionaries/api/lexicala1/
"""

pip install requests

import requests

url = "https://lexicala1.p.rapidapi.com/test"

headers = {
	"X-RapidAPI-Host": "lexicala1.p.rapidapi.com",
	"X-RapidAPI-Key": "7838189de3msh8ee82a3fb123edfp155210jsnf79d3f3429b6"
}
response = requests.request("GET", url, headers=headers)
print(response.text)

url = "https://lexicala1.p.rapidapi.com/search"

querystring = {"text":"güzel","language":"tr"}

headers = {
	"X-RapidAPI-Host": "lexicala1.p.rapidapi.com",
	"X-RapidAPI-Key": "7838189de3msh8ee82a3fb123edfp155210jsnf79d3f3429b6"
}

response = requests.request("GET", url, headers=headers, params=querystring)

print(response.text)

url = "https://lexicala1.p.rapidapi.com/entries/TR_DE00004547"

headers = {
	"X-RapidAPI-Host": "lexicala1.p.rapidapi.com",
	"X-RapidAPI-Key": "7838189de3msh8ee82a3fb123edfp155210jsnf79d3f3429b6"
}

response = requests.request("GET", url, headers=headers)

print(response.text)

url = "https://lexicala1.p.rapidapi.com/senses/TR_SE00007002"

headers = {
	"X-RapidAPI-Host": "lexicala1.p.rapidapi.com",
	"X-RapidAPI-Key": "7838189de3msh8ee82a3fb123edfp155210jsnf79d3f3429b6"
}

response = requests.request("GET", url, headers=headers)

print(response.text)



"""WORD NET KULLANIMI

"""

!git clone https://github.com/StarlangSoftware/TurkishWordNet-Py.git
import os
os.chdir(f'./TurkishWordNet-Py')

!pip3 install NlpToolkit-WordNet

ls

!python setup.py  --requires

!python setup.py --help

!python setup.py build
!python setup.py install

from WordNet.WordNet import WordNet
from WordNet.SynSet import SynSet
from WordNet.Relation import Relation
from WordNet.SemanticRelation import SemanticRelation
from WordNet.SemanticRelationType import SemanticRelationType

ls

domain= WordNet("WordNet/data/turkish_wordnet.xml")

synset=domain.getSynSetWithLiteral("açık",1);
sem=SemanticRelation
for i in range(synset.relationSize()):
  print(synset.getRelation(i));
sem=SemanticRelation(synset.getRelation(0).getName(),SemanticRelationType.ANTONYM);
try:
  sem.__eq__(synset.getRelation(0))
  print("Karşılığı var");
  sys_ant=domain.getSynSetWithId(synset.getRelation(0).getName())
  print(sys_ant.getSynonym().getLiteral(0).getName())
except:
  print("karşılığı yok");

print(domain.getSynSetWithId("TUR10-0169670"));
synset= domain.getSynSetWithId("TUR10-0169670");
rel=Relation("ANTONYM-güzel");
sem=SemanticRelation(rel,SemanticRelationType.ANTONYM);
#synset.addRelation(sem);
if synset.containsRelationType(SemanticRelationType.ANTONYM):
  print("dshdk");
print(synset.getRelation(3).getName());

print(synset.relationSize())
#print(domain.getSynSetsWithLiteral("güzel"))
#for i in domain.getSynSetsWithLiteral("güzel"):
#  print(i)

"""Adj ve adv tespiti Onur Yılmaz"""

!git clone https://github.com/onuryilmaz/turkish-pos-tagger.git
import os
os.chdir(f'./turkish-pos-tagger')

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# ls
# pwd
#

"""Python 2 ile codlanmış dosyaların python 3 çevrilmesi"""

pip install 2to3

cd ..

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# 2to3 --output-dir=python3-version/turkish-pos-tagger/ -W -n turkish-pos-tagger/

ls

cd turkish-pos-tagger/

!rm evaluate_tagger.py
!rm pos_tagger.py
!rm Tagger.py
!rm training_tagger.py

cd ../../..

!cp "/content/python3-version/turkish-pos-tagger/training_tagger.py" "content/turkish-pos-tagger/"
!cp "/content/python3-version/turkish-pos-tagger/evaluate_tagger.py" "content/turkish-pos-tagger/"
!cp "/content/python3-version/turkish-pos-tagger/pos_tagger.py" "content/turkish-pos-tagger/"
!cp "/content/python3-version/turkish-pos-tagger/Tagger.py" "content/turkish-pos-tagger/"

cd content/turkish-pos-tagger/

from google.colab import files

!python training_tagger.py

from pos_tagger import tag

tag('Film çok yavaş ilerlediği için git .')

# Commented out IPython magic to ensure Python compatibility.
# %%bash
# cd /content/turkish-pos-tagger
#

"""Zemberek ile Fiilern Tespiti"""

pip install zemberek-python

from zemberek import TurkishMorphology
morphology = TurkishMorphology.create_with_defaults()
results = morphology.analyze("izledim")
for result in results:
    print(result)
print("\n")

results = morphology.analyze("kopyalamış")
for result in results:
    list =result.format_morpheme_string();
    print(result)
    if list[0].find('Verb'):
      print("yes");

print("\n")

"""BERT MODEL ÇALIŞMASI"""

pip install transformers

from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline

model = AutoModelForSequenceClassification.from_pretrained("savasy/bert-base-turkish-sentiment-cased")
tokenizer = AutoTokenizer.from_pretrained("savasy/bert-base-turkish-sentiment-cased")
sa= pipeline("sentiment-analysis", tokenizer=tokenizer, model=model)

review="film çok iyi ama baya iyi"
r_list=review.lower().split(' ');
print(r_list)
scores=[0.0]*len(r_list);
print(scores)

rm deneme_S.csv

from google.colab import files
uploaded = files.upload()



import io
import pandas as pd
df2 = pd.read_csv(io.BytesIO(uploaded['deneme_S.csv']))
s=dict(zip(df2.B1, df2.B2))
print (s)

def get_key(val):
    for key, value in s.items():
         if val == value:
             return key
    return "key doesn't exist"
r_lists={};
for i in range(len(r_list)):
  if r_list[i] in s:
    r_lists[i]=[s[r_list[i]] if index==i else r_list[index] for index in range(len(r_list))]
  elif r_list[i] in list(s.values()):
    r_lists[i]=[get_key(r_list[i]) if index==i else r_list[index] for index in range(len(r_list))]
  else:
    print("Kelimenin olumsuzu sözlükte bulunamadı.")
print(r_lists)

def listToString(s):
    str1 = " "
    return (str1.join(s))

def calculate(p,p2):
  result_list=[];
  if p[0]['label']=='negative':
    os=0-p[0]['score'];
  else: os=p[0]['score'];
  if p2[0]['label']=='negative':
    cs=0-p2[0]['score'];
  else: cs=p2[0]['score'];
  result=((os-cs)/2);
  return result;

p = sa(review)
print(p)
# [{'label': 'LABEL_1', 'score': 0.9871089}]
print(p[0]['label'] == 'LABEL_1')
# True
for i in r_lists:
  p2 = sa(listToString(r_lists[i]))
  print(listToString(r_lists[i]))
  scores[i]=abs(calculate(p,p2))
print(scores)



# [{'label': 'LABEL_0', 'score': 0.9975505}]
#print(p2[0]['label'] == 'LABEL_1')
# False
#result=(p[0]['score']+p2[0]['score'])/2;
#print(result,"%",result/p[0]['score']*100)

import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import IPython
from IPython.core.display import display, HTML

def colorize(words, color_array):
    if(p[0]['label']=='positive'):
      cmap=matplotlib.cm.Greens
    else:
      cmap=matplotlib.cm.Reds
    template = '<span class="barcode"; style="color: black; background-color: {}">{}</span>'
    colored_string = ''
    for word, color in zip(words, color_array):
        color = matplotlib.colors.rgb2hex(cmap(color)[:3])
        print(color)
        colored_string += template.format(color, '&nbsp' + word + '&nbsp')
    return colored_string

words = r_list
color_array =scores
print(color_array)

print(scores)
rw = colorize(words, color_array)
print(len(rw))
# or simply save in an html file and open in browser
with open('colorize.html', 'w') as f:
    f.write(rw)
IPython.display.HTML(filename="colorize.html")